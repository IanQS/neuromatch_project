{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IanQS/neuromatch_project/blob/main/NMA_2023_IBL_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the external data sources\n",
        "\n",
        "Downloads the data in parallel"
      ],
      "metadata": {
        "id": "uyl7Nqd-j6lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"https://ibl.flatironinstitute.org/public/mainenlab/Subjects/ZFM-01576/2020-12-01/001/alf/probe00/pykilosort/spikes.clusters.9f648cc5-9574-410a-9f5c-717cb5e1f7b8.npy\" >> srcs.txt\n",
        "!echo \"https://ibl.flatironinstitute.org/public/mainenlab/Subjects/ZFM-01576/2020-12-01/001/alf/probe00/pykilosort/spikes.times.cbe0311b-075a-4f3d-a825-ebf5666990a4.npy\" >> srcs.txt\n",
        "\n",
        "!cat srcs.txt | xargs -n 1 -P 2 wget -q\n",
        "\n",
        "!mv \"spikes.times.cbe0311b-075a-4f3d-a825-ebf5666990a4.npy\" spike_times.npy"
      ],
      "metadata": {
        "id": "Lem5ixnbaw39"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "First install the IBL pipeline, which NMA curated to only have the behavioral data, we'll have to import the database"
      ],
      "metadata": {
        "id": "Gtffc_7JcgTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install IBL pipeline package to access and navigate the pipeline\n",
        "!pip install --quiet nma-ibl ONE-api ibllib"
      ],
      "metadata": {
        "id": "Dt-rzX7Qj1fT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure datajoint to link up with the NMA IBL database"
      ],
      "metadata": {
        "id": "tHZrsKbklvDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datajoint as dj\n",
        "dj.config['database.host'] = 'datajoint-public.internationalbrainlab.org'\n",
        "dj.config['database.user'] = 'ibl-public'\n",
        "dj.config['database.password'] = 'ibl-public'\n",
        "\n",
        "from nma_ibl import reference, subject, action, acquisition, data, behavior, behavior_analyses"
      ],
      "metadata": {
        "id": "gQH3lPv9lruH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "N7v9tolwrOkX"
      },
      "outputs": [],
      "source": [
        "#imports here\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "\n",
        "# Turn off logging, this is a hidden cell on docs page\n",
        "import logging\n",
        "logger = logging.getLogger('ibllib')\n",
        "logger.setLevel(logging.CRITICAL)\n",
        "\n",
        "\n",
        "# We should load the data from NMA (IBL Behavior) and the data from IBL themselves\n",
        "# (the cleaned up spikes) below in a way that makes sense\n",
        "#here we run imports for the IBL ephys data\n",
        "from one.api import ONE\n",
        "from brainbox.io.one import SpikeSortingLoader\n",
        "from ibllib.atlas import AllenAtlas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spike Dataset Construction\n",
        "\n",
        "- creates a sliding window of the spikes. We can align these with the responses from the subject"
      ],
      "metadata": {
        "id": "4X0uGk9hwnT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load('spike_times.npy')\n",
        "data.shape\n",
        "\n",
        "def construct_window(flattened_np_arr: np.ndarray, time_interval=0.1, window_length = 25, dataset_size=10):\n",
        "    dataset = []\n",
        "    curr_window = []\n",
        "    start_interval = 0.0\n",
        "    num_iters = 0\n",
        "    pbar = tqdm.tqdm(total=dataset_size)\n",
        "    while len(dataset) < dataset_size:\n",
        "        while len(curr_window) < window_length:\n",
        "            num_spikes = len(flattened_np_arr[np.where((start_interval <= flattened_np_arr) & (flattened_np_arr < start_interval + time_interval))])\n",
        "            start_interval += time_interval\n",
        "            num_iters += 1\n",
        "            curr_window.append(num_spikes)\n",
        "\n",
        "        dataset.append(curr_window)\n",
        "        curr_window = []\n",
        "        if num_iters > 1000:\n",
        "            break\n",
        "        pbar.update(1)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "construct_window(\n",
        "    data, time_interval=0.001, window_length = 10, dataset_size=10\n",
        ")"
      ],
      "metadata": {
        "id": "MznxbyRrwmc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we set up the interface with the ephys database, first we authenticate with the public password, then make a list of **probe IDs** with ```pids```, load a particular **pid** with ```SpikeSortingLoader()```"
      ],
      "metadata": {
        "id": "X12CXlUjrpzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ONE.setup(base_url='https://openalyx.internationalbrainlab.org', silent=True)\n",
        "one = ONE(password='international')\n",
        "# one = ONE()\n",
        "ba = AllenAtlas()\n",
        "pids = [\n",
        "   '1a276285-8b0e-4cc9-9f0a-a3a002978724',\n",
        "   '1e104bf4-7a24-4624-a5b2-c2c8289c0de7',\n",
        "   '5d570bf6-a4c6-4bf1-a14b-2c878c84ef0e',\n",
        "   '5f7766ce-8e2e-410c-9195-6bf089fea4fd',\n",
        "   '6638cfb3-3831-4fc2-9327-194b76cf22e1',\n",
        "   '749cb2b7-e57e-4453-a794-f6230e4d0226',\n",
        "   'd7ec0892-0a6c-4f4f-9d8f-72083692af5c',\n",
        "   'da8dfec1-d265-44e8-84ce-6ae9c109b8bd',\n",
        "   'dab512bd-a02d-4c1f-8dbc-9155a163efc0',\n",
        "   'dc7e9403-19f7-409f-9240-05ee57cb7aea',\n",
        "   'e8f9fba4-d151-4b00-bee7-447f0f3e752c',\n",
        "   'eebcaf65-7fa4-4118-869d-a084e84530e2',\n",
        "   'fe380793-8035-414e-b000-09bfe5ece92a',\n",
        "]\n",
        "pid = pids[0]\n",
        "eid, name = one.pid2eid(pid)\n",
        "\n",
        "sl = SpikeSortingLoader(pid=pid, one=one, atlas=ba)\n",
        "spikes, clusters, channels = sl.load_spike_sorting()\n",
        "clusters = sl.merge_clusters(spikes, clusters, channels)"
      ],
      "metadata": {
        "id": "-A3becQorpK2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading in the data sets we should extract relevant details to structure that's gonna work easier with numpy and scikitlearn.\n",
        "\n",
        "w1d3 notebook loads the Steinmetz data as a dictionary, ```spikes```: an array of normalized spike rates with shape (n_trials, n_neurons), and ```choices```: a vector of 0s and 1s, indicating the animal's behavioural response, with length n_trials.\n",
        "\n"
      ],
      "metadata": {
        "id": "yK3FoSWD83ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#squish data into something easy to work with, a dictionary of arrays works"
      ],
      "metadata": {
        "id": "-_mqCNaurX3Y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then just train a model and instead of just taking vanilla accuracy we can use cross-validation"
      ],
      "metadata": {
        "id": "PrtRPME99g7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##   accuracies = cross_val_score(LogisticRegression(penalty=None), spikes, choices, cv=8)  # k=8 cross validation"
      ],
      "metadata": {
        "id": "tfMZedHM8jfe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can imagine we're gonna have WAY more features than samples if we take each neuron to be a feature"
      ],
      "metadata": {
        "id": "Y7px-7Kv-eWF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5pP-DV3N-tBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}