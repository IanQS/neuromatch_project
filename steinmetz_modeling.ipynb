{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOytzXq0AsYk4bOjTeeCeI8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IanQS/neuromatch_project/blob/main/steinmetz_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Parameters\n",
        "\n",
        "Modify based on what you want to do with the dataset construction"
      ],
      "metadata": {
        "id": "4diB7ZzrcOef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class WindowChoice(enum.Enum):\n",
        "    START = 0\n",
        "    MID = 1\n",
        "    END = 2\n",
        "\n",
        "\n",
        "WINDOW_SIZE = 50\n",
        "TRAIN_TEST_SPLIT = 0.8  # 80% is training, 20% test\n",
        "SHUFFLE_DATASET = True\n",
        "\n",
        "assert 0 < TRAIN_TEST_SPLIT <= 1.0\n",
        "\n",
        "\n",
        "DATASET_PARAMETERS = dict()\n",
        "\n",
        "DATASET_PARAMETERS[\"window_choice\"] = WindowChoice.END\n",
        "DATASET_PARAMETERS[\"window_size\"] = WINDOW_SIZE\n",
        "DATASET_PARAMETERS[\"train-test-split\"] = TRAIN_TEST_SPLIT\n",
        "DATASET_PARAMETERS[\"shuffle\"] = SHUFFLE_DATASET"
      ],
      "metadata": {
        "id": "Tc59dMcyaGYg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling of the Steinmetz dataset\n",
        "\n",
        "- uses [Neuromatch Load Steinmetz Decisions](https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/main/projects/neurons/load_steinmetz_decisions.ipynb#scrollTo=DJ-jzsE5eLxX) as a base"
      ],
      "metadata": {
        "id": "IDojqX3X1w30"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "K0fUDdS41qWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a997a917-f7cd-43bc-edf8-1af2a326a7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.41 ms (started: 2023-07-21 04:51:34 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore\n",
        "from sklearn.decomposition import PCA\n",
        "import concurrent.futures\n",
        "from multiprocessing import Pool\n",
        "from typing import Dict, List, Any\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# !pip install -q ipython-autotime\n",
        "# %load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Data Downloading And Stacking\n",
        "import os, requests\n",
        "\n",
        "fname = []\n",
        "for j in range(3):\n",
        "  fname.append('steinmetz_part%d.npz'%j)\n",
        "url = [\"https://osf.io/agvxh/download\"]\n",
        "url.append(\"https://osf.io/uv3mw/download\")\n",
        "url.append(\"https://osf.io/ehmw2/download\")\n",
        "\n",
        "for j in range(len(url)):\n",
        "  if not os.path.isfile(fname[j]):\n",
        "    try:\n",
        "      r = requests.get(url[j])\n",
        "    except requests.ConnectionError:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      if r.status_code != requests.codes.ok:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "      else:\n",
        "        with open(fname[j], \"wb\") as fid:\n",
        "          fid.write(r.content)\n",
        "\n",
        "all_ds = np.array([])\n",
        "for j in range(len(fname)):\n",
        "  all_ds = np.hstack((all_ds,\n",
        "                      np.load('steinmetz_part%d.npz'%j,\n",
        "                              allow_pickle=True)['dat']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNH_c8UKVK84",
        "outputId": "c3844b1a-f51c-40a3-fc1f-50102e4f25f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 52.2 s (started: 2023-07-21 03:26:30 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Description\n",
        "\n",
        "(taken and modified from the Neuromatch Load Steinmetz Decisions notebook)\n",
        "\n",
        "## High-level\n",
        "\n",
        "`all_ds` contains 39 sessions from 10 mice, data from Steinmetz et al, 2019. Time bins for all measurements are 10ms, starting 500ms before stimulus onset. The mouse had to determine which side has the highest contrast. For each `curr_ds = all_ds[k]`, you have the fields below. For extra variables, check out the extra notebook and extra data files (lfp, waveforms and exact spike times, non-binned).\n",
        "\n",
        "## Fields Used\n",
        "\n",
        "* `curr_ds['spks']`: neurons by trials by time bins.    \n",
        "* `curr_ds['brain_area']`: brain area for each neuron recorded.\n",
        "* `curr_ds['response']`: which side the response was (`-1`, `0`, `1`). When the right-side stimulus had higher contrast, the correct choice was `-1`. `0` is a no go response.\n",
        "\n",
        "## Fields present (not all are used)\n",
        "\n",
        "* `curr_ds['mouse_name']`: mouse name\n",
        "* `curr_ds['date_exp']`: when a session was performed\n",
        "* `curr_ds['ccf']`: Allen Institute brain atlas coordinates for each neuron.\n",
        "* `curr_ds['ccf_axes']`: axes names for the Allen CCF.\n",
        "* `curr_ds['contrast_right']`: contrast level for the right stimulus, which is always contralateral to the recorded brain areas.\n",
        "* `curr_ds['contrast_left']`: contrast level for left stimulus.\n",
        "* `curr_ds['gocue']`: when the go cue sound was played.\n",
        "* `curr_ds['response_time']`: when the response was registered, which has to be after the go cue. The mouse can turn the wheel before the go cue (and nearly always does!), but the stimulus on the screen won't move before the go cue.  \n",
        "* `curr_ds['feedback_time']`: when feedback was provided.\n",
        "* `curr_ds['feedback_type']`: if the feedback was positive (`+1`, reward) or negative (`-1`, white noise burst).  \n",
        "* `curr_ds['wheel']`: turning speed of the wheel that the mice uses to make a response, sampled at `10ms`.\n",
        "* `curr_ds['pupil']`: pupil area  (noisy, because pupil is very small) + pupil horizontal and vertical position.\n",
        "* `curr_ds['face']`: average face motion energy from a video camera.\n",
        "* `curr_ds['licks']`: lick detections, 0 or 1.   \n",
        "* `curr_ds['trough_to_peak']`: measures the width of the action potential waveform for each neuron. Widths `<=10` samples are \"putative fast spiking neurons\".\n",
        "* `curr_ds['%X%_passive']`: same as above for `X` = {`spks`, `pupil`, `wheel`, `contrast_left`, `contrast_right`} but for  passive trials at the end of the recording when the mouse was no longer engaged and stopped making responses.\n",
        "* `curr_ds['prev_reward']`: time of the feedback (reward/white noise) on the previous trial in relation to the current stimulus time.\n",
        "* `curr_ds['reaction_time']`: ntrials by 2. First column: reaction time computed from the wheel movement as the first sample above `5` ticks/10ms bin. Second column: direction of the wheel movement (`0` = no move detected).  \n",
        "\n",
        "\n",
        "The original dataset is here: https://figshare.com/articles/dataset/Dataset_from_Steinmetz_et_al_2019/9598406"
      ],
      "metadata": {
        "id": "6R9YTk2nlj57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regions = [\"vis ctx\", \"thal\", \"hipp\", \"other ctx\", \"midbrain\", \"basal ganglia\", \"cortical subplate\", \"other\"]\n",
        "region_colors = ['blue', 'red', 'green', 'darkblue', 'violet', 'lightblue', 'orange', 'gray']\n",
        "brain_groups = [[\"VISa\", \"VISam\", \"VISl\", \"VISp\", \"VISpm\", \"VISrl\"],  # visual cortex\n",
        "                [\"CL\", \"LD\", \"LGd\", \"LH\", \"LP\", \"MD\", \"MG\", \"PO\", \"POL\", \"PT\", \"RT\", \"SPF\", \"TH\", \"VAL\", \"VPL\", \"VPM\"], # thalamus\n",
        "                [\"CA\", \"CA1\", \"CA2\", \"CA3\", \"DG\", \"SUB\", \"POST\"],  # hippocampal\n",
        "                [\"ACA\", \"AUD\", \"COA\", \"DP\", \"ILA\", \"MOp\", \"MOs\", \"OLF\", \"ORB\", \"ORBm\", \"PIR\", \"PL\", \"SSp\", \"SSs\", \"RSP\",\"TT\"],  # non-visual cortex\n",
        "                [\"APN\", \"IC\", \"MB\", \"MRN\", \"NB\", \"PAG\", \"RN\", \"SCs\", \"SCm\", \"SCig\", \"SCsg\", \"ZI\"],  # midbrain\n",
        "                [\"ACB\", \"CP\", \"GPe\", \"LS\", \"LSc\", \"LSr\", \"MS\", \"OT\", \"SNr\", \"SI\"],  # basal ganglia\n",
        "                [\"BLA\", \"BMA\", \"EP\", \"EPd\", \"MEA\"]  # cortical subplate\n",
        "                ]\n",
        "\n",
        "# Assign each area an index\n",
        "area_to_index = dict(root=0)\n",
        "counter = 1\n",
        "for group in brain_groups:\n",
        "    for area in group:\n",
        "        area_to_index[area] = counter\n",
        "        counter += 1\n",
        "\n",
        "# Figure out which areas are in each dataset\n",
        "areas_by_dataset = np.zeros((counter, len(all_ds)), dtype=bool)\n",
        "for j, d in enumerate(all_ds):\n",
        "    for area in np.unique(d['brain_area']):\n",
        "        i = area_to_index[area]\n",
        "        areas_by_dataset[i, j] = True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-TrPD1RaSdE",
        "outputId": "0e13cb5c-366d-44ea-d5a3-aa768775d97d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 12.9 ms (started: 2023-07-21 03:27:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_IDX = 11\n",
        "curr_ds = all_ds[DATASET_IDX]\n",
        "\n",
        "dt = curr_ds[\"bin_size\"]\n",
        "NUM_NEURONS_RECORDED = curr_ds[\"spks\"].shape[0]\n",
        "NUM_TRIALS = curr_ds[\"spks\"].shape[1]\n",
        "NUM_BINNED_TIMES = curr_ds[\"spks\"].shape[2]\n",
        "\n",
        "if DATASET_IDX != 11:\n",
        "    raise Exception(\"Code is only meant for DATASET_IDX=11\")\n",
        "else:\n",
        "    NUM_REGIONS = 4\n",
        "    NUM_NEURONS_RECORDED = len(curr_ds[\"brain_area\"])  # The string idx version of\n",
        "\n",
        "brain_subregions = NUM_REGIONS * np.ones(NUM_NEURONS_RECORDED, )  # last one is \"other\"\n",
        "for j in range(NUM_REGIONS):\n",
        "  brain_subregions[\n",
        "      np.isin(curr_ds['brain_area'], brain_groups[j])\n",
        "      ] = j  # assign a number to each region\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKkzVgrtaUIK",
        "outputId": "448ff6c3-e0ae-439f-a06c-21c40bd78814"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.66 ms (started: 2023-07-21 03:27:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the dataset\n",
        "\n",
        "1) Create the labels\n",
        "\n",
        "2) Create a dataset dictionary where the keys are brain areas (sub-regions) and the values are all the neuron readings that are in that area/sub-region\n",
        "\n",
        "3) Enable users to specify their config of how they want the data: do we consider region interactions, should we consider the start/middle/end of the spike train, etc."
      ],
      "metadata": {
        "id": "YWUD2SeXvb79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = curr_ds[\"response\"]  # RIGHT - NO_GO - LEFT (-1, 0, 1)\n",
        "y = LABELS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdR8dZCikHnf",
        "outputId": "a9f82b43-ea46-4484-c0e8-5090e52ccb81"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 544 µs (started: 2023-07-21 03:27:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_shapes(ds):\n",
        "    _ds = ds['spks']\n",
        "    print(f\"All spikes shape: {_ds.shape}\")\n",
        "    _ds_brain_region = _ds[brain_subregions == 0]\n",
        "    print(f\"\\t- Spike shape for sample brain region (0-th): {_ds_brain_region.shape}\")\n",
        "\n",
        "    _ds_0th_left_response = _ds_brain_region[:, y >= 0]\n",
        "    print(f\"\\t- Spike shape for sample brain region (0-th) left responses: {_ds_0th_left_response.shape}\")\n",
        "\n",
        "    averaged_over_left_response = _ds_0th_left_response.mean(axis=(0, 1))\n",
        "    print(f\"\\t- Averaged brain region (0-th) left responses: {averaged_over_left_response.shape}\")\n",
        "\n",
        "log_shapes(curr_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-ydT0MKi6zg",
        "outputId": "06104287-43a2-4141-e89a-ba418a4e0783"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All spikes shape: (698, 340, 250)\n",
            "\t- Spike shape for sample brain region (0-th): (145, 340, 250)\n",
            "\t- Spike shape for sample brain region (0-th) left responses: (145, 199, 250)\n",
            "\t- Averaged brain region (0-th) left responses: (250,)\n",
            "time: 27.2 ms (started: 2023-07-21 03:27:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the fine-grained data dictionary"
      ],
      "metadata": {
        "id": "M_qjuS2TorDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_by_subregion(arr_of_subregions: List[str], ds: Dict[str, Any]) -> Dict[str, List[np.ndarray]]:\n",
        "    spike_partitioned = {}  # brain region to spike mapping\n",
        "    unique_subregions = set(arr_of_subregions)\n",
        "    for subregion in unique_subregions:\n",
        "        subregion_idxs = arr_of_subregions == subregion\n",
        "        subregion_data = ds[\"spks\"][subregion_idxs]\n",
        "\n",
        "\n",
        "        # from the \"Dataset Description\" section above\n",
        "        #       > which side the response was (-1, 0, 1)\n",
        "        spikes_for_right_response = subregion_data[:, y < 0]\n",
        "        spikes_for_left_response = subregion_data[:, y > 0]\n",
        "\n",
        "        # spikes_for_no_response = subregion_data[:, y == 0]\n",
        "\n",
        "        spike_partitioned[subregion] = [\n",
        "            spikes_for_left_response,\n",
        "            # spikes_for_no_response,\n",
        "            spikes_for_right_response\n",
        "        ]\n",
        "    return spike_partitioned\n",
        "\n",
        "subregion_data_dict = dataset_by_subregion(curr_ds[\"brain_area\"], curr_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukGSq1aDibUS",
        "outputId": "f49c581b-17b9-404d-eb9a-b60dc40ac92d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 66.5 ms (started: 2023-07-21 04:11:44 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Neurons recorded in each subregion \")\n",
        "running_sum = 0\n",
        "for k, v in subregion_data_dict.items():\n",
        "    print(f\"\\t{k}\\t {v[0].shape[0]}\")\n",
        "    running_sum += v[0].shape[0]\n",
        "\n",
        "assert running_sum == curr_ds[\"spks\"].shape[0], \"Our totaled neurons across all subregions are not equal to the number of neurons measured\"\n",
        "print(running_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48y6Yqb4t5Up",
        "outputId": "f1e653fd-9857-4669-db1e-14bf7b7e4aff"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Neurons recorded in each subregion \n",
            "\tLH\t 18\n",
            "\tMD\t 126\n",
            "\troot\t 100\n",
            "\tLGd\t 11\n",
            "\tMOs\t 6\n",
            "\tCA1\t 50\n",
            "\tVISam\t 79\n",
            "\tVISp\t 66\n",
            "\tACA\t 16\n",
            "\tPL\t 56\n",
            "\tDG\t 65\n",
            "\tSUB\t 105\n",
            "698\n",
            "time: 2.71 ms (started: 2023-07-21 04:11:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Coarse-Grained Data Dictionary\n",
        "\n",
        "- we do this manually since we do not have too many subregions\n",
        "\n",
        "### All regions\n",
        "```python\n",
        "[\"VISa\", \"VISam\", \"VISl\", \"VISp\", \"VISpm\", \"VISrl\"],  # visual cortex\n",
        "[\"CL\", \"LD\", \"LGd\", \"LH\", \"LP\", \"MD\", \"MG\", \"PO\", \"POL\", \"PT\", \"RT\", \"SPF\", \"TH\", \"VAL\", \"VPL\", \"VPM\"], # thalamus\n",
        "[\"CA\", \"CA1\", \"CA2\", \"CA3\", \"DG\", \"SUB\", \"POST\"],  # hippocampal\n",
        "[\"ACA\", \"AUD\", \"COA\", \"DP\", \"ILA\", \"MOp\", \"MOs\", \"OLF\", \"ORB\", \"ORBm\", \"PIR\", \"PL\", \"SSp\", \"SSs\", \"RSP\",\"TT\"],  # non-visual cortex\n",
        "[\"APN\", \"IC\", \"MB\", \"MRN\", \"NB\", \"PAG\", \"RN\", \"SCs\", \"SCm\", \"SCig\", \"SCsg\", \"ZI\"],  # midbrain\n",
        "[\"ACB\", \"CP\", \"GPe\", \"LS\", \"LSc\", \"LSr\", \"MS\", \"OT\", \"SNr\", \"SI\"],  # basal ganglia\n",
        "[\"BLA\", \"BMA\", \"EP\", \"EPd\", \"MEA\"]  # cortical subplate\n",
        "```\n",
        "\n",
        "\n",
        "### Refined Regions\n",
        "\n",
        "- only the ones relevant to our dataset\n",
        "\n",
        "```python\n",
        "MD -> thalamus\n",
        "ACA -> non-visual-cortex\n",
        "SUB -> hippocampal\n",
        "CA1 -> hippocampal\n",
        "DG -> hippocampal\n",
        "LGd -> thalamus\n",
        "LH -> thalamus\n",
        "PL -> non-visual-cortex\n",
        "root ->\n",
        "VISp -> visual-cortex\n",
        "MOs -> non-visual-cortex\n",
        "VISam -> visual-cortex\n",
        "```"
      ],
      "metadata": {
        "id": "mNg0_VE7ot00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(subregion_data_dict[\"MD\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npoicE1niZsf",
        "outputId": "4d9baf83-f40a-42be-e98f-f84de844f4eb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.9 ms (started: 2023-07-21 04:11:48 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Manual Insertion"
      ],
      "metadata": {
        "id": "eLACF0PTYoie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def consolidate_fine_grained(subregion_dict):\n",
        "\n",
        "    mapping = {\n",
        "        \"thalamus\": [\"MD\", \"LGd\", \"LH\"],\n",
        "        \"non-visual-cortex\": [\"ACA\", \"PL\", \"MOs\"],\n",
        "        \"hippocampal\": [\"SUB\", \"CA1\", \"DG\"],\n",
        "        \"visual-cortex\": [\"VISp\", \"VISam\"]\n",
        "    }\n",
        "\n",
        "\n",
        "    coarse_region_data_dict: Dict[str, List[List[np.ndarray]]] = dict()\n",
        "\n",
        "    for coarse_region_name, subregion_name_arr in mapping.items():\n",
        "        print(\"*\" * 10)\n",
        "        print(coarse_region_name)\n",
        "        for subregion_name in subregion_name_arr:\n",
        "            print(f\"Subregion: {subregion_name}\")\n",
        "            if coarse_region_name not in coarse_region_data_dict:\n",
        "                print(f\"\\tInit: Left and Right: {subregion_dict[subregion_name][0].shape}, {subregion_dict[subregion_name][1].shape}\")\n",
        "                coarse_region_data_dict[coarse_region_name] = copy.deepcopy(subregion_dict[subregion_name])\n",
        "            else:\n",
        "                subregion_left, subregion_right = subregion_dict[subregion_name]\n",
        "\n",
        "                print(f\"\\tIncoming Shapes: Left and Right: {subregion_left.shape}, {subregion_right.shape}\")\n",
        "                # print(f\"Container: {coarse_region_data_dict[coarse_region_name][1]}\")\n",
        "                coarse_region_data_dict[coarse_region_name][0] = np.vstack(\n",
        "                    (coarse_region_data_dict[coarse_region_name][0],\n",
        "                    subregion_left)\n",
        "                )\n",
        "                coarse_region_data_dict[coarse_region_name][1] = np.vstack(\n",
        "                    (coarse_region_data_dict[coarse_region_name][1],\n",
        "                    subregion_right)\n",
        "                )\n",
        "            print(f\"\\tPost-stack shapes: {coarse_region_data_dict[coarse_region_name][0].shape} {coarse_region_data_dict[coarse_region_name][1].shape}\")\n",
        "    return coarse_region_data_dict\n",
        "\n",
        "coarse_region_data_dict = consolidate_fine_grained(subregion_data_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ9Vo6TzoyhU",
        "outputId": "466ceb63-e5b0-4b13-e4d5-52d43ce880ed"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********\n",
            "thalamus\n",
            "Subregion: MD\n",
            "\tInit: Left and Right: (126, 135, 250), (126, 141, 250)\n",
            "\tPost-stack shapes: (126, 135, 250) (126, 141, 250)\n",
            "Subregion: LGd\n",
            "\tIncoming Shapes: Left and Right: (11, 135, 250), (11, 141, 250)\n",
            "\tPost-stack shapes: (137, 135, 250) (137, 141, 250)\n",
            "Subregion: LH\n",
            "\tIncoming Shapes: Left and Right: (18, 135, 250), (18, 141, 250)\n",
            "\tPost-stack shapes: (155, 135, 250) (155, 141, 250)\n",
            "**********\n",
            "non-visual-cortex\n",
            "Subregion: ACA\n",
            "\tInit: Left and Right: (16, 135, 250), (16, 141, 250)\n",
            "\tPost-stack shapes: (16, 135, 250) (16, 141, 250)\n",
            "Subregion: PL\n",
            "\tIncoming Shapes: Left and Right: (56, 135, 250), (56, 141, 250)\n",
            "\tPost-stack shapes: (72, 135, 250) (72, 141, 250)\n",
            "Subregion: MOs\n",
            "\tIncoming Shapes: Left and Right: (6, 135, 250), (6, 141, 250)\n",
            "\tPost-stack shapes: (78, 135, 250) (78, 141, 250)\n",
            "**********\n",
            "hippocampal\n",
            "Subregion: SUB\n",
            "\tInit: Left and Right: (105, 135, 250), (105, 141, 250)\n",
            "\tPost-stack shapes: (105, 135, 250) (105, 141, 250)\n",
            "Subregion: CA1\n",
            "\tIncoming Shapes: Left and Right: (50, 135, 250), (50, 141, 250)\n",
            "\tPost-stack shapes: (155, 135, 250) (155, 141, 250)\n",
            "Subregion: DG\n",
            "\tIncoming Shapes: Left and Right: (65, 135, 250), (65, 141, 250)\n",
            "\tPost-stack shapes: (220, 135, 250) (220, 141, 250)\n",
            "**********\n",
            "visual-cortex\n",
            "Subregion: VISp\n",
            "\tInit: Left and Right: (66, 135, 250), (66, 141, 250)\n",
            "\tPost-stack shapes: (66, 135, 250) (66, 141, 250)\n",
            "Subregion: VISam\n",
            "\tIncoming Shapes: Left and Right: (79, 135, 250), (79, 141, 250)\n",
            "\tPost-stack shapes: (145, 135, 250) (145, 141, 250)\n",
            "(155, 135, 250) (155, 141, 250)\n",
            "time: 43.9 ms (started: 2023-07-21 04:11:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Construction"
      ],
      "metadata": {
        "id": "hxSTyVM6XZy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def populate_data(designed_matrix, is_left, X_container, y_container):\n",
        "    \"\"\"\n",
        "    designed_matrix is of shape (a, b, c)\n",
        "        a:= num_neurons in coarse_region\n",
        "        b:= num_trials  (in this case either the left or right response trials)\n",
        "        c:= spike_train\n",
        "    \"\"\"\n",
        "    for trials_matrix in designed_matrix:\n",
        "        for spike_train in trials_matrix:\n",
        "            X_container.append(spike_train)\n",
        "            y_container.append(1 if is_left else -1)\n",
        "    return X_container, y_container\n",
        "\n",
        "def encode_coarse_data(\n",
        "    coarse_data_dict: Dict[str, List[List[np.ndarray]]],\n",
        "):\n",
        "    unique_keys = dict()\n",
        "    one_hot_idx = 0\n",
        "\n",
        "    X_container = []\n",
        "    y_container = []\n",
        "    for coarse_region_name, coarse_region_data in coarse_data_dict.items():\n",
        "        print(\"*\" * 20)\n",
        "        print(coarse_region_name)\n",
        "        # Enumerate all of the arrays of the subregions and vertically stack them\n",
        "        left = coarse_region_data[0]   # The positive label (left) of our LGd, for example\n",
        "        right = coarse_region_data[1]  # The negative label (right) of our LH, for example\n",
        "        _l_shape = left.shape\n",
        "        _r_shape = right.shape\n",
        "\n",
        "        assert _l_shape[-1] == 250\n",
        "        assert _r_shape[-1] == 250\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "        # Add 1-hot encoded data\n",
        "        #   For more information: https://en.wikipedia.org/wiki/One-hot\n",
        "\n",
        "        vec_one_hot = [0 for _ in range(len(coarse_region_data_dict.keys()))]\n",
        "        vec_one_hot[one_hot_idx] = 1\n",
        "\n",
        "        left_pad = np.tile(vec_one_hot, (_l_shape[0], _l_shape[1], 1))\n",
        "        print(left_pad.shape)\n",
        "        right_pad = np.tile(vec_one_hot, (_r_shape[0], _r_shape[1], 1))\n",
        "        left_designed = np.concatenate((left_pad, left), axis=-1)\n",
        "        right_designed = np.concatenate((right_pad, right), axis=-1)\n",
        "\n",
        "        print(f\"Shape B4 populating: {np.asarray(X_container).shape}, {np.asarray(y_container).shape}\")\n",
        "        X_container, y_container = populate_data(left_designed, True, X_container, y_container)\n",
        "        X_container, y_container = populate_data(right_designed, False, X_container, y_container)\n",
        "        print(f\"Shape After populating: {np.asarray(X_container).shape}, {np.asarray(y_container).shape}\")\n",
        "\n",
        "        # We are now in a new region, so we increment the index for the one-hot\n",
        "        one_hot_idx += 1\n",
        "\n",
        "    return np.asarray(X_container), np.asarray(y_container)\n",
        "\n",
        "\n",
        "Xs, ys = encode_coarse_data(\n",
        "    coarse_region_data_dict,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B-hL3DCYtDP",
        "outputId": "be47b06a-fbc3-4d4c-a67f-d3cf3f603264"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************\n",
            "thalamus\n",
            "(155, 135, 4)\n",
            "Shape B4 populating: (0,), (0,)\n",
            "Shape After populating: (42780, 254), (42780,)\n",
            "********************\n",
            "non-visual-cortex\n",
            "(78, 135, 4)\n",
            "Shape B4 populating: (42780, 254), (42780,)\n",
            "Shape After populating: (64308, 254), (64308,)\n",
            "********************\n",
            "hippocampal\n",
            "(220, 135, 4)\n",
            "Shape B4 populating: (64308, 254), (64308,)\n",
            "Shape After populating: (125028, 254), (125028,)\n",
            "********************\n",
            "visual-cortex\n",
            "(145, 135, 4)\n",
            "Shape B4 populating: (125028, 254), (125028,)\n",
            "Shape After populating: (165048, 254), (165048,)\n",
            "time: 1.66 s (started: 2023-07-21 04:33:53 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXSRVPABwuh4",
        "outputId": "030a5a1b-ea8f-405e-e7c0-0160c30f4b81"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(165048, 254)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 22.6 ms (started: 2023-07-21 04:43:55 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Configuration\n",
        "\n",
        "Here we use the `DATASET_PARAMETERS` that was specified above"
      ],
      "metadata": {
        "id": "Wp7vLufeu4B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_dataset(dataset_parameters, X_data, y_data):\n",
        "    \"\"\"\n",
        "    DATASET_PARAMETERS[\"window_choice\"] = WindowChoice.END\n",
        "    DATASET_PARAMETERS[\"window_size\"] = WINDOW_SIZE\n",
        "    DATASET_PARAMETERS[\"train-test-split\"] = TRAIN_TEST_SPLIT\n",
        "    \"\"\"\n",
        "\n",
        "    ##############################################\n",
        "    # First step is we extract the spike train window of interest based on whether we want the start, mid, or end\n",
        "    window_choice = dataset_parameters[\"window_choice\"]\n",
        "\n",
        "    if window_choice == WindowChoice.START:\n",
        "        start = 0\n",
        "        end = start + dataset_parameters[\"window_size\"]\n",
        "    elif window_choice == WindowChoice.MID:\n",
        "        start = X_data.shape[-1] // 2\n",
        "        end = start + dataset_parameters[\"window_size\"]\n",
        "    else:\n",
        "        start = (X_data.shape[-1] - 1) - dataset_parameters[\"window_size\"]\n",
        "        end = (X_data.shape[-1] - 1)\n",
        "\n",
        "    new_Xs = []\n",
        "    for row in X_data:\n",
        "        new_Xs.append(row[start:end])\n",
        "    X_data = np.asarray(new_Xs)\n",
        "\n",
        "    ##############################################\n",
        "    # Optionally shuffle the dataset\n",
        "    if dataset_parameters[\"shuffle\"]:\n",
        "        X_data, y_data = shuffle(X_data, y_data)\n",
        "\n",
        "\n",
        "    ##############################################\n",
        "    # Next step is to split into train-test\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=dataset_parameters[\"train-test-split\"], random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "create_dataset(DATASET_PARAMETERS, Xs, ys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifAvUl5Qu_v6",
        "outputId": "77eab027-4ef0-4136-cf42-c1f6376a0889"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]),\n",
              " array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [1, 0, 1, ..., 0, 0, 1],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]),\n",
              " array([ 1,  1,  1, ...,  1, -1, -1]),\n",
              " array([ 1, -1, -1, ..., -1,  1, -1]))"
            ]
          },
          "metadata": {},
          "execution_count": 71
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 416 ms (started: 2023-07-21 04:49:34 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RQCud45GxuLX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}